\documentclass[12pt]{article}
\usepackage{times}
\usepackage[hungarian,british]{babel}
\usepackage{url}
\usepackage{latexsym}
\usepackage[utf8]{inputenc}
\usepackage[hidelinks]{hyperref}
\usepackage{color}
\usepackage[dvipsnames]{xcolor}
\definecolor{OliveGreen}{rgb}{0,0.6,0}
\usepackage{graphicx}
\usepackage{booktabs,amsmath,multicol,bm}
\usepackage{breakcites}
\usepackage{float}
\usepackage[nottoc]{tocbibind}
\setcounter{secnumdepth}{3}
\newcommand{\tab}[1]{\hspace{.2\textwidth}\rlap{#1}}
\usepackage[top=2.5cm, bottom=4cm, left=2.5cm, right=2.5cm]{geometry}

\begin{document}
	\title{Deep Learning Based Chatbot Models}
	\author{Richárd Krisztián Csáky \\ Budapest University of Technology and Economics}
	\date{\today}
	
\begin{titlepage}
		\centering
		\includegraphics[width=0.6\textwidth]{pics/bme_logo_nagy.eps}\par\vspace{1cm}
		{\scshape Budapest University of Technology and Economics \\ Faculty of Electrical Engineering and Informatics \\
			Department of Automation and Applied Informatics \par}
		\vspace{2cm}
		{\huge\mdseries Deep Learning Based Chatbot Models\par}
		\vspace{0.5cm}
		{\scshape\Large Scientific Students' Associations Report\par}
		\vspace{1.5cm}
		
		{\Large Author: \\ Richárd Krisztián Csáky\par}
		\vspace{1.5cm}
		{\Large Supervised by \\ Gábor Recski\par}
		\vfill
		\par
		{\Large 2017\par}
		
		
\end{titlepage}

\begin{otherlanguage}{hungarian}
\begin{abstract}
A konverzációs ágens (chatbot) egy olyan program, mely természetes nyelvet használva képes emberekkel kommunikálni. A beszélgetés modellezése fontos feladat a természetes nyelvfeldolgozás és mesterséges intelligencia (MI) területén. Az MI tudományág megszületése óta egy jól működő chatbot létrehozása még mindig az egyik legnehezebb kihívás. A chatbotok sokféle feladatra használhatók, de mindegyik esetében elvárt, hogy megértsék a felhasználó mondandóját és az adott problémához releváns válaszokat generáljanak.
		
A múlt chatbot architektúrái kézi szabályokra és sablonokra, vagy egyszerű statisztikai módszerekre támaszkodtak. 2015 óta, a  mélytanulás (deep learning) elterjedésével ezek a modellek gyorsan felcserélődtek elejétől végéig tanítható neurális hálózatokkal. Manapság a rekurrens enkóder-dekóder modell \cite{Cho:2014} dominál a konverzáció modellezésben. Ezt az architektúrát a neurális gépi fordítás területéről adaptálták, ahol rendkívül jó eredményeket ért el. Azóta sokféle változata \cite{Serban:2015} és kiegészítése született annak érdekében, hogy minél jobb minőségű legyen a chatbotok által folytatott beszélgetés.
		
Munkám során részletes irodalmi kutatást végeztem, melyben az elmúlt 3 évben publikált, több mint 70, a chatbotokkal kapcsolatos publikációt vizsgálok meg. Ezután amellett érvelek, hogy a konverzáció modellezés sajátosságai a jelenlegi state-of-the-art architektúráktól eltérő megközelítést igényelnek. Szakirodalmi példákon alapulva bemutatom, hogy a jelenlegi chatbot modellek miért nem vesznek figyelembe elég ún. priort a válasz generálása során, és ez hogyan befolyásolja a beszélgetés minőségét. Ezek a priorok olyan külső információt hordoznak, melyen a beszélgetés kondicionálva lehet, mint például a beszélők személye \cite{Li:2016} vagy hangulata. Amellett, hogy bemutatom az okait, javaslatokat is teszek a probléma orvoslására.
		
A dolgozat következő részében egy nemrég bemutatott modellt, mely jelenleg state-of-the-art-nak számít a neurális gépi fordításban, az úgynevezett Transformer-t \cite{Vaswani:2017} adaptálom a beszélgetés-modellezés feladatára. Először az eredeti cikkben leírt modell tanításával kísérletezek, tanítóadatként a Cornell Movie-Dialog Corpus \cite{Danescu:2011} dialógusait használva. Emellett továbbfejlesztem a modellt saját, az enkóder-dekóder architektúra hiányainak orvoslására született ötletekkel. További priorokat adok bemenetként a modellbe, mint a beszélgetők személye vagy hangulata. Végül korábbi chatbot modellekkel való összehasonlítás útján részletes elemzést végzek arról, hogy az eredeti modell mennyire teljesít jól dialógus adattal és hogyan befolyásolják a generált válaszok minőségét az általam implementált további kiegészítések.
\end{abstract}
\end{otherlanguage}


\newpage\begin{abstract}
A conversational agent (chatbot) is a piece of software that is able to communicate with humans using natural language. Modeling conversation is an important task in natural language processing and artificial intelligence (AI). Indeed, ever since the birth of AI, creating a good chatbot remains one of the field’s hardest challenges. While chatbots can be used for various tasks, in general they have to understand users’ utterances and provide responses that are relevant to the problem at hand.

In the past, methods for constructing chatbot architectures have relied on hand-written rules and templates or simple statistical methods. With the rise of deep learning these models were quickly replaced by end-to-end trainable neural networks around 2015. More specifically, the recurrent encoder-decoder model \cite{Cho:2014} dominates the task of conversational modeling. This architecture was adapted from the neural machine translation domain, where it performs extremely well. Since then a multitude of variations \cite{Serban:2015} and features were presented that augment the quality of the conversation that chatbots are capable of.

In my work, I conduct an in-depth survey of recent literature, examining over 70 publications related to chatbots published in the last 3 years. Then I proceed to make the argument that the very nature of the general conversation domain demands approaches that are different from current state-of-the-art architectures. Based on several examples from the literature I show why current chatbot models fail to take into account enough priors when generating responses and how this affects the quality of the conversation. In the case of chatbots these priors can be outside sources of information that the conversation is conditioned on like the persona \cite{Li:2016} or mood of the conversers. In addition to presenting the reasons behind this problem, I propose several ideas on how it could be remedied.

The next section of my paper focuses on adapting the very recent Tranformer \cite{Vaswani:2017} model to the chatbot domain, which is currently the state-of-the-art in neural machine translation. I first present my experiments with the vanilla model, using conversations extracted from the Cornell Movie-Dialog Corpus \cite{Danescu:2011}. Secondly, I augment the model with some of my ideas regarding the issues of encoder-decoder architectures. More specifically, I feed additional features into the model like mood or persona together with the raw conversation data. Finally, I conduct a detailed analysis of how the vanilla model performs on conversational data by comparing it to previous chatbot models and how the additional features, affect the quality of the generated responses.
\end{abstract}

\newpage\tableofcontents
\newpage\section{Introduction} \label{sec:intro}

A conversational agent (chatbot) is a piece of software that is able to communicate with humans using natural language. Ever since the birth of AI, modeling conversations remains one of the field's toughest challenges. Even though they are far from perfect, chatbots are now used in a plethora of applications like Apple's Siri \cite{Siri:2017}, Google's Google Assistant \cite{Google:2017} or Microsoft's Cortana \cite{Cortana:2017}. In order to fully understand the capabilities and limitations of current chatbot architectures and techniques I conduct an in-depth survey, where I examine related literature published over the past 3 years and I implement my own chatbot based on a novel neural network model. 

The paper begins with a brief overview of the history of chatbots in Section~\ref{sec:history}, where I discuss the properties and objectives of conversational modeling. I present early approaches in Section~\ref{ssec:22} and the current dominating model based on neural networks for building conversational agents in Section~\ref{ssec:23}.

In Section~\ref{sec:background} I present key architectures and techniques that were developed over the past 3 years relating to chatbots. I group publications into groups based on the specific techniques or approaches that are discussed by the authors. After this I present criticism in Section~\ref{ssec:problems} regarding some of the properties of current chatbot models and I show how several of the techniques used are inappropriate for the task of modeling conversations.

In the next part (Section~\ref{sec:experiments}) I conduct experiments by training a novel neural network based model, the Transformer \cite{Vaswani:2017} using dialog datasets \cite{Danescu:2011,Tiedemann:2009,OpenSubtitles:2016}. I run several trainings using these datasets detailed in Section~\ref{ssec:43}. I present the results of the different training setups in Section~\ref{sec:results} by qualitatively comparing them to previous chatbot models and by using standard evaluation metrics.

Finally in Section~\ref{sec:future} before concluding in Section~\ref{sec:conclusion} I offer possible directions for future work. More specifically, I propose several ideas in order to remedy the problems discussed in Section~\ref{ssec:problems}.

\newpage\section{History of Chatbots} \label{sec:history}

\subsection{Modeling Conversations} \label{ssec:21}
Chatbot models usually take in as input natural language sentences uttered by the user, and output a response. There are two main approaches for generating responses. The traditional approach is to use hard-coded templates and rules to make chatbots, which I present in Section~\ref{ssec:22}. The more novel approach, which I discuss in detail in Section~\ref{ssec:23} was made possible by the rise of deep learning. Neural network models are trained on large amounts of data to learn the process of generating relevant and grammatically correct responses to input utterances. Models have also been developed to accommodate for spoken or visual inputs. They oftentimes make use of a speech recognition component to transform speech into text \cite{Serban:2017} or convolutional neural networks that transform the input pictures into useful representations for the chatbot \cite{Havrylov:2017}. The latter models are also called visual dialog agents, where the conversation is grounded on both textual and visual input \cite{Das:2017}.

Conversational agents exist in two main forms. The first one is the more traditional task-oriented dialog system, which is limited in its conversational capabilities, however it is very robust at executing task specific commands and requirements. Task-oriented models are built to accomplish a specific task like making restaurant reservations \cite{Joshi:2017,Bordes:2016} or promoting movies \cite{Yu:2017}, just to name a few. These systems often don't have the ability to respond to arbitrary utterances since they are limited to a specific domain, thus users have to be guided by the dialog system towards the task at hand. Usually they are deployed to tasks where some information has to be retrieved from a knowledge base. They are mainly used to replace the process of navigating through menus and user interfaces like making the process of booking flight tickets or finding a public transportation route between two locations conversational \cite{Zhao:2017}.  

The second type of dialog agents are the non-task or open-domain chatbots. These conversation systems try to imitate human dialog in all its facets. This means that one should hardly be able to distinguish such a chatbot from a real human, but current models are still far away from such claims. These models are usually trained with conversation examples extracted from movie scripts or from Twitter-like post-reply pairs \cite{Vinyals:2015,Shang:2015,Serban:2015,Li:2016}. For these models there isn't a well defined goal, but they are required to have a certain amount of world knowledge and commonsense in order to hold conversations about basically anything.

Recently an emphasis has been put on integrating the two types of conversational agents. The main idea is to combine the positive aspects of both types, like the robust abilities of goal-oriented dialog systems to perform tasks and the human-like chattyness of open-domain chatbots \cite{Zhao:2017,Yu:2017,Serban:2017}. This is beneficial because the user is more likely to engage with a task-oriented dialog agent if its more human-like, and handles out of domain responses well.

\subsection{Early Approaches} \label{ssec:22}

ELIZA is one of the first ever chatbot programs written \cite{Weizenbaum:1966}. It uses clever hand-written templates to generate a reply that resembles the user's input utterance. Since then countless hand-coded rule-based chatbots have been written \cite{Wallace:2009,Cleverbot:2017,Mitsuku:2017}. Furthermore, a number of programming frameworks specifically designed to facilitate building dialog agents have been developed \cite{Marietto:2013,Microsoft:2017}.

\begin{figure}[H]
	\label{fig:22a}
	\centering
	\includegraphics[width=0.4\textwidth]{pics/cleverbot.png}
	\caption{Sample conversation with Cleverbot \cite{Cleverbot:2017}}
\end{figure}


These chatbot programs are very similar in their core, namely that they all use hand-written rules to generate replies. Usually simple pattern matching or keyword retrieval techniques are employed to handle the user's input utterance. Then rules are used to transform a matching pattern or a keyword into a predefined reply.
A simple example is shown below in AIML \cite{Marietto:2013}:\\
{\color{OliveGreen}\textless category\textgreater}

{\color{OliveGreen}\textless pattern\textgreater}What is your name?{\color{OliveGreen}\textless/pattern\textgreater}

{\color{OliveGreen}\textless template\textgreater}My name is Alice{\color{OliveGreen}\textless/template\textgreater}\\
{\color{OliveGreen}\textless/category \textgreater}\\
\\
Here if the input sentence matches the sentence written between the \textit{\textless pattern\textgreater} brackets the reply written between the \textit{\textless template\textgreater} brackets is outputted.\\
Another example is shown below where we make use of the \textit{star} symbol to replace words. In this case whatever word follows the word \textit{like} it will be present in the response at the position specified by the \textit{\textless star/\textgreater} token:\\
{\color{OliveGreen}\textless category\textgreater}

{\color{OliveGreen}\textless pattern\textgreater}I like *{\color{OliveGreen}\textless/pattern\textgreater}

{\color{OliveGreen}\textless template\textgreater}I too like {\color{OliveGreen}\textless star/\textgreater}.{\color{OliveGreen}\textless/template\textgreater}\\
{\color{OliveGreen}\textless/category \textgreater}\\

\subsection{The Encoder-Decoder Model} \label{ssec:23}
The main concept that differentiates rule-based and neural network based approaches is the presence of a learning algorithm in the latter case. An important distinction has to be made between traditional machine learning and deep learning which is a sub-field of the former. In my work I talk about deep learning methods applied to chatbots since neural networks have been the backbone of the field of conversational modeling and traditional machine learning methods are only rarely used as supplementary techniques.   

When applying neural networks to natural language processing (NLP) tasks we have to transform each word (symbol) into a numerical representation \cite{Bengio:2003}. This is done through word embeddings, which represent each word as a fixed size vector of real numbers. Word embeddings are useful because instead of handling words as huge vectors of the size of the vocabulary we can represent them in much lower dimensions. I talk about the vocabulary used in NLP tasks in more detail in Section~\ref{sssec:234}. The vector that represents each word can be regarded as a set of parameters and these parameters can be jointly learned with the neural network's parameters, or they can be pre-learned which I detail further in Section~\ref{sssec:pretrain}.

Instead of using hand-written rules deep learning models transform input sentences into replies directly by using matrix multiplications and non-linear functions that contain millions of parameters. We can further divide neural network based conversational models into two categories, retrieval-based and generative models. The former simply returns a reply from the dataset by computing the most likely response to the current input utterance based on a scoring function, which can be implemented as a neural network \cite{Cho:2014} or by simply computing the cosine similarity between the input utterance and candidate replies \cite{stalemate:2016}. Generative models on the other hand synthesize the reply one word at a time by computing probabilities over the whole vocabulary \cite{Sutskever:2014,Vinyals:2015}. There have also been approaches that integrate the two types of dialog systems by comparing a generated reply with a retrieved reply and determining which one is more likely to be a better response \cite{Song:2016}.

As with many other applications the field of conversational modeling has been transformed by the rise of deep learning. More specifically the encoder-decoder recurrent neural network (RNN) model (also called seq2seq \cite{Sutskever:2014}) introduced by \cite{Cho:2014} and its variations have been dominating the field. After talking about RNNs in Section~\ref{sssec:231} I present the seq2seq model in Section~\ref{sssec:232}. This model was originally developed for neural machine translation (NMT), but it was found to be suitable to \textit{translate} source utterances into responses within a conversational setting \cite{Shang:2015,Vinyals:2015}. Even though this is a relatively new field already attempts to create unified dialog platforms for training and evaluating different conversational models have been proposed \cite{Miller:2017}.

\subsubsection{Recurrent Neural Networks} \label{sssec:231}
A recurrent neural network (RNN) \cite{RNN:1988} is a neural network that can take as inputs a variable length sequence \(\bm{x}=(\bm{x}_1,...,\bm{x}_n)\), by unfolding itself over each input \(\bm{x}_i\) and generating a hidden state at each step \(\bm{h}=(\bm{h_1},...,\bm{h_n})\). At each step \(i\), the hidden state \(\bm{h}_i\) is updated by
\begin{equation} \label{eq231a}
\bm{h}_i=f(W\bm{h}_{i-1}+U\bm{x}_i)
\end{equation}
which is also called the unrolling of the network. \(W\) and \(U\) are matrices containing the weights (parameters) of the network. \(f\) here is a non-linear activation function which can be the hyperbolic tangent function for example. The vanilla implementation of an RNN is rarely used, because it suffers from the vanishing gradient problem which makes it very hard to train \cite{Hochreiter:1998}. Usually long short-term memory (LSTM) \cite{Hochreiter:1997} or gated recurrent units (GRU) \cite{Cho:2014} are used for the activation function. LSTMs were developed to combat the problem of long-term dependencies that vanilla RNNs face. As the number of steps of the unrolling increase it becomes increasingly hard for a simple RNN to learn to remember information seen multiple steps ago. 

LSTM networks use a gating mechanism to control the information flow in the recurrent network. More specifically the input and forgetting gates are used to determine how to update the network's state and the output gate is used to determine what to output. Mathematically these gates consist of several matrix multiplications and non-linear functions applied to the input vector and the previous hidden state. LSTMs are particularly useful for language modeling, because we need to preserve information over multiple sentences while the network is unrolled over each word. Because of space constraints I don't detail LSTMs here further, but a very good quick explanation can be found in \cite{LSTM_article}. An important characteristic of recurrent neural networks is that the parameters of the function \(f\) don't change during the unrolling of the network.

\begin{figure}[H]
	\label{fig:231}
	\centering
	\includegraphics[width=0.5\textwidth]{pics/rnn.jpg}
	\caption{Unfolding of an RNN over 3 time-steps. Here \(x\) is the input sequence, \(o\) is the output sequence, \(s\) is the sequence of hidden states and \(U\),\(W\) and \(V\) are the weights of the network. \cite{RNN_pic:2017}}
\end{figure}

Language modeling is the task of predicting the next word in a sentence based on the previous words \cite{Bengio:2003}. RNNs can be used for language modeling by training them to learn the probability distribution over the vocabulary \(\bm{V}\) given an input sequence of words. As previously discussed the RNN receives the word embedding vectors representing words in lower dimensions. We can generate the probability distribution to predict the next word in the sequence by taking the hidden state of the RNN in the last step, and feeding it into a softmax activation function
\begin{equation} \label{eq231b}
p(\bm{x}_{i,j}|\bm{x}_{i-1},...,\bm{x}_1)=\frac{exp(\bm{v}_j\bm{h}_i)}{\sum_{j^{'}=1}^{K}exp(\bm{v}_{j^{'}}\bm{h}_i)}
\end{equation}
for all possible words (symbols) \(j=1,...,K\), where \(\bm{v}_j\) are the rows in the \(V\) weight matrix of the softmax function. \((\bm{x}_{i-1},...,\bm{x}_1)\) is the input sequence and \(\bm{h}_i\) is the hidden state of the RNN at step \(i\).

Training of these networks is done via the generalized backpropagation algorithm called truncated backpropagation through time \cite{Werbos:1990,RNN:1988}. Essentially we backpropagate the error through each time step of the network to learn its parameters. The error can be computed by using the cross-entropy loss, which calculates how different the predictions are compared to the true labels.
\begin{equation} \label{eq231c}
L(\bm{y}_i,\bm{\hat{y}}_{i})=-\bm{y}_i log(\bm{\hat{y}}_{i})
\end{equation}
where \(\bm{\hat{y}}_{i}\) is the vector of the predicted probabilities over all words in the vocabulary at step \(i\), and \(\bm{y}_i\) is the one-hot vector over the vocabulary. A one-hot vector is made up of zeros except at the index of the one true word that follows in the sequence, where it is equal to 1. After computing the derivative with respect to all of the weights in the network using the backpropagation through time algorithm we can update the weights in order to get closer to an optimum with optimization techniques like stochastic gradient descent (SGD) \cite{SGD:2010}. 

\subsubsection{The Seq2seq Model} \label{sssec:232}
The sequence to sequence model (seq2seq) was first introduced by \cite{Cho:2014}, but they only used it to re-rank sentences instead of generating completely new ones, which was first done by \cite{Sutskever:2014}. Since then, besides NMT and conversational models a plethora of different applications of these models have been introduced like text summarization \cite{Nallapati:2016}, speech recognition \cite{Chiu:2017}, code generation \cite{Rico:2017} and parsing \cite{Konstas:2017}, just to name a few.

\begin{figure}[H]
	\label{fig:232a}
	\centering
	\includegraphics[width=0.9\textwidth]{pics/seq2seq.png}
	\caption{A general seq2seq model, where \((A,B,C)\) is the input sequence, \(<EOS>\) is a symbol used to delimit the end of the sentence and \((W,X,Y,Z)\) is the output sequence \cite{Sutskever:2014}}
\end{figure}

The simplest and initial form of the model is based on two RNNs. The actual implementation of RNNs can be in the form of LSTMs or GRUs as discussed in Section~\ref{sssec:231}. The goal is to estimate the conditional probability of \(p(\bm{y}_1,...,\bm{y}_{N^{'}}|\bm{x}_1,...,\bm{x}_N)\), where \(\bm{x}_1,...,\bm{x}_N\) is the input sequence and \(\bm{y}_1,...,\bm{y}_{N^{'}}\) is the corresponding output sequence. Since two different RNNs are used for the input and output sequences, the length of the sequences, \(N\) and \(N^{'}\) can be different. The encoder RNN is unrolled over the words in the source sequence and its last hidden state is called the thought vector, which is a representation of the whole source sequence. The initial hidden state of the decoder RNN is then set to this representation \(\bm{v}\), and the generation of target sequence words is done by taking the output of the unrolled decoder network at each time-step and feeding it into a softmax function, which produces the probabilities over all words in the vocabulary.
\begin{equation} \label{eq232a}
p(\bm{y}_1,...,\bm{y}_{N^{'}}|\bm{x}_1,...,\bm{x}_N)=\prod_{i=1}^{N^{'}}p(\bm{y}_i|\bm{v},\bm{y}_1,...,\bm{y}_{i-1})
\end{equation}

Training is very similar to a normal RNN, namely the log probability of a correct target sequence \(T\) given the source sequence \(S\) is maximized
\begin{equation} \label{eq232b}
\frac{1}{\bm{S}}\sum_{T,S \in \bm{S}}log(p(T|S))
\end{equation}
where \(\bm{S}\) is the training set. The two networks are trained jointly, errors are backpropagated through the whole model and the weights are optimized with some kind of optimization technique like SGD.

In NMT the input sequences are sentences in one language from which we wish to and target sequences are sentences in a different language to which we wish to translate. The individual elements of a sequence or sentence in this case are vectors representing word embeddings. In conversational modeling the simplest approach is to treat an utterance by a speaker as input sequence and the response to that utterance from a different speaker as the target sequence. I will talk about better approaches however in Section~\ref{sssec:context}.
\subsubsection{Deep Seq2seq Models}
Seq2seq models can also contain multiple layers of LSTM networks as seen in Figure~\ref{fig:232b}. This is done in order to make the model deeper and to have more parameters, which should ideally lead to better performance \cite{Vinyals:2015,googleNMT:2016}. There exist multiple variants, but the most straightforward one is to feed in the source sentence to the first layer of the encoder network. The output from the previous LSTM layer is fed as input to the next layer and the layers are unrolled jointly. Then the last hidden state of the final encoder layer can be used to initialize the initial hidden state of the first decoding layer. The output of the previous decoder layer is input to the next layer until the final layer, where we apply a softmax activation function over the outputs from the last layer to generate the predicted target sequence. How we initialize the layers in the decoder network can be implemented in many different ways, like taking the last hidden state from each encoder layer and using it to initialize the first hidden state of each corresponding decoder layer, to name another method.

\begin{figure}[H]
	\label{fig:232b}
	\centering
	\includegraphics[width=0.55\textwidth]{pics/deep_seq2seq.png}
	\caption{A 3 layer seq2seq model \cite{deep_seq2seq}. The lines pointing from the last decoder states to the last encoder represent an attention mechanism about which I talk in Section~\ref{sssec:attention}}
\end{figure}

\subsubsection{Decoding and Vocabulary} \label{sssec:234}
In order to get the actual generated output sequences there are several techniques to decode from the probabilities of the words. One such technique is the Roulette Wheel selection, which is commonly used for genetic algorithms \cite{GA:1998}. Using this method, at each time-step each word from the vocabulary can be generated with the probability computed from the softmax function. This is useful if we want to have some stochasticity in the decoding process, because it can produce slightly different outputs for the same source sequence. However it doesn't perform as well as the more frequent method used, which is to simply output the word with the highest probability from the softmax function. This is a greedy and deterministic approach since we will always get the same output for the same input.

While decoding a word at each time-step is fine a better approach is to the decode the whole output sequence at the same time, by outputting the sequence with the highest probability.
\begin{equation} \label{eq233}
\hat{T}=\arg\max_{T}p(T|S)
\end{equation}
Here \(S\) is the source sentence and \(T\) is the target sentence. Since in order to get the sequence with the highest probability we have to first generate all of the possible sequences a simple left-to-right beam search is usually employed to make the computation tractable. First, \(K\) words with the highest probability are retained in the first time-step. Then at each time-step we expand this list by computing the joint probability of the partial sequences in the list and the words in the current time-step and retaining the \(K\) most probable partial sequences until we get to the end of the output sequence.

Another important aspect of sequence-to-sequence models applied to tasks involving language is the vocabulary. The vocabulary consists of all the different words and symbols present in the dataset. One problem with this approach is that the vocabulary tends to be quite large, especially for very big datasets like \cite{OpenSubtitles:2016,opensubtitles}. Since the number of parameters of the model increases proportionally with the size of the vocabulary it is usually the case that the vocabulary is limited to some arbitrary size \(K\). This way we only use the embeddings of the \(K\) most frequent words in the dataset and replace any other symbols with a common token representing unknown words. Many approaches have been proposed to the problem of handling out of vocabulary (OOV) or unknown words \cite{Luong:2014,Feng:2017,Jean:2014}. Other methods involve using characters \cite{Zhu:2017} or subword units \cite{Sennrich:2015} instead of words.

\newpage\section{Background} \label{sec:background}
In this section I will first present selected publications from my literature research. I categorize these papers into several categories, each corresponding to a specific approach or aspect of conversational modeling. In Section~\ref{ssec:31} I describe further details regarding encoder-decoder models and in Section~\ref{ssec:32} I give an overview of several techniques used to augment the performance of encode-decoder models. In Section~\ref{ssec:33} I present different approaches to conversational modeling that aren't based on the original seq2seq model.

Finally, in Section~\ref{ssec:problems} I present some criticism regarding basic techniques used in neural conversation models. While they are widely used, I show that the assumptions on which their usage rests are generally wrong and  in consequence these methods are unsuitable for modeling conversations. Finally I provide a brief summary of the section.

\subsection{Further Details of Encoder-Decoder Models} \label{ssec:31}
In this section I describe some further details regarding seq2seq models. First, I talk about the context of conversations and how it can be taken into account in Section~\ref{sssec:context}. Then I present different objective functions that can be used to train seq2seq models in Section~\ref{sssec:functions}. Finally I describe methods used for evaluating conversational agents in Section~\ref{sssec:eval}.

\subsubsection{Context} \label{sssec:context}
% In context section talk about simply feeding the whole conversation to encoder
In this section I first describe a commonly used variation of the RNN called the bidirectional RNN (BiRNN) \cite{Schuster:1997}. A BiRNN consists of two RNNs, a forward and a backward one. The forward RNN reads the input sequence in the original order (from \(\bm{x}_1\) to \(\bm{x}_n\)) and computes a sequence of forward hidden states \((\overrightarrow{\bm{h}}_1,...,\overrightarrow{\bm{h}}_n)\). The backward RNN reads the reversed sequence (from \(\bm{x}_n\) to \(\bm{x}_1\)) and computes the backward hidden states \((\overleftarrow{\bm{h}}_1,...,\overleftarrow{\bm{h}}_n)\). Then we can simply concatenate the two hidden states for each word in the sequence \cite{Bahdanau:2014,Zhaob:2017}. The BiRNN resembles a bag of words model, because each concatenated hidden state \(\bm{h}_i\) has information about the words surrounding the \(i\)-th word. This results in a better preservation of context and is used in several seq2seq models, usually in the encoder network \cite{Zhaob:2017,Xing_topic:2017,googleNMT:2016,Yin:2017}.
\begin{figure}[H]
	\label{fig:context}
	\centering
	\includegraphics[width=1.0\textwidth]{pics/bilstm.png}
	\caption{A bidirectional multilayer LSTM encoder and unidirectional LSTM decoder with attention \cite{Yin:2017}}
\end{figure}
So far I have only described conversational modeling as predicting a reply based on a single source utterance. However conversations are more complicated than machine translation, since they don't solely consist of utterance-reply pairs. Conversations usually have many turns, and the reply to an utterance might depend on information presented in previous turns of the conversation. A plethora of approaches have been proposed to incorporate context or conversation history into seq2seq models in order to build better dialog agents. Perhaps the most straightforward approach is to concatenate \(k\) previous utterances by appending an end-of-utterance symbol after each utterance and feeding this long sequence of symbols into the encoder \cite{Vinyals:2015}. The simplest approach which was used as a baseline in \cite{Sordoni:2015} is to use only the first preceding utterance as context in order to form context-message-reply triples for training. A better approach was to concatenate the bag of words representations of the context and message utterances instead of the actual utterances. By using different representations for different utterances better results were achieved \cite{Sordoni:2015}.

However feeding long sequences of symbols into the encoder RNN of seq2seq models accentuates the vanishing gradient problem \cite{Hochreiter:1998}. Also as sequences get longer it becomes increasingly more difficult for RNNs to retain information which was inputted many time-steps ago. Consequently, it becomes even harder to encode all relevant information into the last hidden state of the encoder network from a sequence consisting of multiple sentences.

A possible solution is to represent the current utterance and previous dialog turns with different RNNs and to build a hierarchical representation of the conversation history \cite{Serban:2015}, which I describe in detail in Section~\ref{sssec:HRED}. A similar approach was presented in \cite{Zhaob:2017} using one RNN to encode the current utterance and a different RNN to encode \(k\) previous utterances, but this approach doesn't take into consideration the hierarchical nature of conversations.
 

\subsubsection{Objective Functions} \label{sssec:functions}
In addition to the standard cross-entropy loss introduced in Section~\ref{sssec:231} there have been proposed several different loss functions for conversational models in order to generate more varied and interesting replies. One such class of loss functions are reinforcement learning based, which I talk about in detail in Section~\ref{sssec:RL}. In \cite{Ramachandran:2016} a seq2seq model is pretrained as a language model and in order to avoid over-fitting monolingual language modeling losses are added to the standard loss function. These losses act as a regularizer by forcing the seq2seq model to correctly model both the normal seq2seq task and the original monolingual language modeling tasks. I talk about pretraining in more detail in Section~\ref{sssec:pretrain}. In \cite{Lison:2017} it is argued that not all context-response pairs in a dialog dataset have the same importance for conversational modeling. Accordingly they weight each context-response pair with a scoring model and use these weights in the loss function when training a seq2seq model. Hence, examples associated with bigger weights in the dataset will have a larger impact on the gradient update steps through the loss function. 

Approaches to incorporate beam search into the training procedure have also been explored. In \cite{Wiseman:2016} a simple approach is taken to construct the loss function by penalizing when the gold target sequence is not present in the beam consisting of the most likely predictions. The issue with incorporating beam search directly into the training procedure of seq2seq models is that it uses the argmax function which is discontinuous and hence is not amenable to backpropagation. Since the predictions, which serve as input to the standard loss function are discrete (from beam search), the evaluation of the final loss is also discontinuous. A relaxation based technique is proposed in \cite{Goyal:2017}, where the beam search based loss function is gradually relaxed to the standard loss function as training progresses.

Perhaps the most extensively used objective function besides the standard one is based on Maximum Mutual Information (MMI) \cite{Li:2015}. In MMI the goal is defined as maximizing pairwise or mutual information between a source sentence \(S\) and the corresponding target \(T\):
\begin{equation}
\log{\frac{p(S,T)}{p(S)p(T)}}=\log{p(T|S)}-\log{p(T)}
\end{equation}
By introducing a weighting constant \(\lambda\) and by using the Bayes' theorem the training objective can be written in the following two ways:
\begin{equation} \label{eqMMIa}
\hat{T}=\arg\max_{T}[\log{p(T|S)}-\lambda\log{p(T)}]
\end{equation}
\begin{equation} \label{eqMMIb}
\hat{T}=\arg\max_{T}[(1-\lambda)\log{p(T|S)}+\lambda\log{p(S|T)}]
\end{equation}
However, it is not trivial to use these equations as loss functions in practice. In Equation~\ref{eqMMIa} the term \(\lambda\log{p(T)}\) acts as an anti-language model since it is subtracted from the loss function. This leads to ungrammatical output sentences in practice. Training using Equation~\ref{eqMMIb} is possible since we just have to train an additional seq2seq model for the \(\log{p(S|T)}\) term by using targets as inputs and inputs as targets, but direct decoding is intractable since it requires completion of target generation before \(p(S|T)\) can actually be computed. Solutions to these issues have been proposed in \cite{Li:2015}.

\subsubsection{Evaluation Methods} \label{sssec:eval}
Evaluation of dialog systems is perhaps a more controversial topic. Indeed it is still an open research problem to find good automatic evaluation metrics that can effectively compare the performance of conversational models. The traditional approach is to use the same metrics that are used for NMT and language modeling. Bleu and perplexity are widely used metrics for evaluating dialog systems \cite{Vinyals:2015,Yao:2016,Zhao:2017,Serban:2015}. 

Bleu \cite{Papineni:2002} measures how many n-word-sequence (n is usually 4) overlaps are there between the test sentences and the predicted ones by the model. Similar to this but less commonly used is the simple accuracy of the whole sentence or the word error rate, which is prevalent for speech recognition models \cite{Shannon:2017,Park:2008} and calculates how many words are incorrect in the predicted sentence.

Perplexity \cite{Manning:1999} is another common metric, which measures how well a probability model predicts a sample. Given a model we can plug sentences that it hasn't seen during training and compute the perplexity on the test set. If we have a model \(q\) (encoder-decoder for conversational modeling), we can compute the perplexity by
\begin{equation}
2^{-\frac{1}{N}\sum_{i=1}^{N}\log_2{q(\bm{x}_i)}}
\end{equation}
where \(x_i\) are the test samples or words in the sentence and \(N\) is the length of the sentence. Essentially we want our models to give very high probability to test samples, meaning that the lower the perplexity score the better.

Unfortunately it has been shown that these metrics don't correlate well with human judgment in the conversational setting \cite{Liu:2016}. I talk about the downfalls of these standard metrics in Section~\ref{sssec:metrics}. A number of different methods have been proposed to address the problem of evaluating conversational agents, for example how many turns it takes until the model produces a generic answer \cite{Zhao:2017,Li_RL:2016} or the diversity of the generated responses \cite{Li_RL:2016}. More sophisticated metrics make use of neural network based models to assign a score to utterance-response pairs and it has been shown that they correlate better with human judgments than traditional metrics \cite{Lowe:2017,Tao:2017}.

For reinforcement learning and task-oriented dialog agents, which I describe in Section~\ref{sssec:RL} and Section~\ref{sssec:task} respectively evaluation is more straightforward. In reinforcement learning we specify a goal that the dialog agent has to achieve, and we can simply measure its performance based on the percent of cases in which it achieves the goal \cite{Li_adversarial:2017,Havrylov:2017}. Similarly for task-oriented dialog agents usually there is a clearly defined task and the accuracy of accomplishing the given task serves as a good performance metric \cite{Joshi:2017,Zhao:2017,Li_HIL:2016}.

Finally, one of the better methods to evaluate dialog systems is to ask other people what they think about the quality of the responses that an agent produces. Human judgment has been one of the most prevalent metrics throughout recent literature related to conversational modeling \cite{Shang:2015,Vinyals:2015,Zhou:2017,Li_RL:2016,Zhao:2017,Li_RL:2016,Li:2015}. Usually human judges are asked to either rate the quality of individual responses from a model on a scale or to choose the better response from responses produced by different models for the same input. A number of properties of conversations can be rated, like naturalness, grammaticality, engagement or simply the overall quality of the conversation.

\subsection{Augmentations To The Encoder-Decoder Model} \label{ssec:32}
In this section I present recent methods and techniques used to augment the performance of encoder-decoder models. In Section~\ref{sssec:attention} I describe different attention mechanisms. Furthermore I show how pretraining of seq2seq models can help in a conversational setting in Section~\ref{sssec:pretrain}. Then I talk about different features and priors that can be used as additional inputs to seq2seq models in Section~\ref{sssec:priors}. Finally I show how knowledge bases and information retrieval can be good additions to encoder-decoder models in Section~\ref{sssec:KB}.

\subsubsection{Attention} \label{sssec:attention}
% max mention attention is all you need here, because it will be described in ssec:41
The attention mechanism was first introduced to encoder-decoder models by \cite{Bahdanau:2014}. The problem that it was trying to address is the limited information that the context vector can carry. In a standard seq2seq model we expect that this single fixed-size vector can encode all the relevant information about the source sentence that the decoder needs in order to generate its prediction. Additionally, since we use a single vector, all the decoder states receive the same information, instead of feeding in information relevant to the specific decoding step. 


\begin{figure}[H]
	\label{fig:attentiona}
	\centering
	\includegraphics[width=0.3\textwidth]{pics/bahdanau_attn.png}
	\caption{Original Attention Mechanism \cite{Bahdanau:2014}}
\end{figure}
In order to combat these shortcomings the attention mechanism creates an additional input at each decoding step coming directly from the encoder states. This additional input \(\bm{c}_i\) to the decoder at time-step \(i\) is computed by taking a weighted sum over the encoder hidden states \(\bm{h}\):
\begin{equation} \label{eqattention}
\bm{c}_i=\sum_{j=1}^{T}a_{ij}\bm{h}_j
\end{equation}
where \(T\) is the number of hidden states or symbols in the source sentence and the weight \(a_{ij}\) for each \(\bm{h}_j\) hidden state can be computed by
\begin{equation}
a_{ij}=\frac{exp(e_{ij})}{\sum_{k=1}^{T}exp(e_{ik})}
\end{equation}
which is basically a softmax function over \(e_{ij}\), which is the output of a scoring function:
\begin{equation}
e_{ij}=f(\bm{s}_{i-1},\bm{h}_j).
\end{equation}
Here \(\bm{s}_{i-1}\) is the hidden state of the decoder in the previous time-step. Oftentimes \(\bm{s}_{i-1}\) is called a query vector \(\bm{q}\) and the encoder hidden states \(\bm{h}\) are called key vectors \(\bm{k}\).
The scoring function \(f\) can be implemented in several ways:
\begin{equation}
MLP(\bm{q},\bm{k})=\bm{w_2}tanh(W_1[\bm{q}:\bm{k}])
\end{equation}
where we simply concatenate the query and key vectors and feed them into a single or multilayer neural network \cite{Bahdanau:2014}. \(\bm{w}_2\) and \(W_1\) are the weights of the network.
\begin{equation}
BL(\bm{q},\bm{k})=\bm{q}^\mathsf{T}W\bm{k}
\end{equation}
is called the bilinear scoring function where \(W\) is a learned weight matrix. \cite{Luong:2015}.
\begin{equation}
DP(\bm{q},\bm{k})=\bm{q}^\mathsf{T}\bm{k}
\end{equation}
is the dot product scoring function, which doesn't use any weights, but it requires the sizes of the two vectors to be the same \cite{Luong:2015}.
\begin{equation}
SDP(\bm{q},\bm{k})=\frac{\bm{q}^\mathsf{T}\bm{k}}{\sqrt{|\bm{k}|}}
\end{equation}
is the scaled dot product, where we scale the previous dot product by the square root of the size of the key vectors \cite{Vaswani:2017}. This is useful because otherwise the dot product increases as dimensions get larger.

We can attend to outputs from the decoder network generated in previous time-steps by incorporating them as additional inputs to the scoring functions mentioned above \cite{Shao:2017}. This helps the decoder by allowing it to \textit{see} what has already been generated.

Attention can also be implemented using multiple heads \cite{Vaswani:2017}, meaning that we use the output of multiple scoring functions, each with their own parameters to learn to focus on different parts of the input sequence. The parameters of the scoring functions are jointly learned with all other parameters of the seq2seq model.



\begin{figure}[H]
	\label{fig:attentionb}
	\centering
	\includegraphics[width=0.4\textwidth]{pics/attn_weights.png}
	\caption{Pixels represent the weights \(a_{ij}\) between inputs and outputs \cite{Bahdanau:2014}}
\end{figure}
Attention can also be computed using only one sequence of symbols \cite{Cheng:2016,Lin:2017,Vaswani:2017}. In this case the query and key vectors both come from the same hidden states and thus we get an encoding of the sentence which depends on all of its parts. This is called self or intra-attention and it can be used both in the decoder and the encoder networks since it can be implemented as a standalone layer that takes in a sequence and computes a new representation of that sequence. 
\begin{figure}[H]
	\label{fig:attentionc}
	\centering
	\includegraphics[width=0.7\textwidth]{pics/self_attn.png}
	\caption{Visualizing self-attention, lines represent the weights \(a_{ij}\) \cite{Vaswani:2017}}
\end{figure}

Attention has been extensively used for neural conversation models as an augmentation of the base seq2seq model \cite{Yao:2016,Shang:2015,Xing_topic:2017,Zhao:2017}. A more complex type of attention mechanism is the hierarchical attention which was used in conversational modeling \cite{Xing:2017} and abstractive text summarization \cite{Nallapati:2016} as well. It is useful when we need to attend over multiple input sentences. I describe it in more detail in Section~\ref{sssec:HRED}.

\subsubsection{Pretraining} \label{sssec:pretrain}
Pretraining for seq2seq models means that instead of initializing the parameters of a model randomly we pretrain the model on some data or some other task that is different from the main task that we want to apply our model to. One of the most common approaches among many NLP tasks is to pretrain the parameters of the word embeddings \cite{Chen:2014,Serban:2015,Akasaki:2017,Lample:2016,Serban:2017}. The most popular techniques to pretrain word embeddings are presented in these papers \cite{Mikolov:2013,Mikolov_skipgram:2013}. An advantage of pretraining word embeddings is that during the actual training of the seq2seq model we can fix these embeddings and thus the model has to learn less parameters. The reason why this works in practice is that word embeddings are just representations of words, hence these parameters don't take an active part in task specific learning, like NMT or conversational modeling.

In addition to pretraining word embeddings we can also pretrain all of the parameters of an encoder-decoder model. For conversational modeling this is very beneficial, because oftentimes well labeled datasets are relatively small. By pretraining on a big but noisier dataset the parameters of the model will already achieve somewhat good performance. Thus the model will have learned a good amount of general knowledge about the task \cite{Li:2016,Serban:2015}. Then by finetuning on the smaller dataset it can achieve better performance without overfitting this smaller dataset. For example a conversational model can learn general knowledge like answering with yes or no to a yes or no question. Then during finetuning the model doesn't have to learn all of this knowledge again and thus can focus on more subtle properties of conversation or domain-specific knowledge. In \cite{Li:2016} a seq2seq model was pretrained on the OpenSubtitles dataset \cite{OpenSubtitles:2016} and then adapted the pretrained model to a much smaller TV series dataset. A somewhat different approach was employed in \cite{Ramachandran:2016}, where the authors pretrained the encoder and decoder RNNs of a seq2seq model separately as language models. Thus both networks already had some knowledge about language before putting them together and training the whole seq2seq model for NMT. Similarly in \cite{Sriram:2017} a pretrained language model (LM) was used to augment the performance of standard seq2seq model. With the information from the LM the seq2seq model was able to improve its performance for domain adaptation, which means that it performed almost as good on a different domain as on the domain it was trained on.

In \cite{Lowe:2017} the authors made an attempt at building a model which automatically assigns scores to sample conversations. To do this they used the encoder and context RNN part of a HRED network (which I talk about in Section~\ref{sssec:HRED}) to encode the conversation. Since their dataset of scored conversations was small they resorted to pretrain the HRED with the normal task of generating replies to conversations.

Lastly, a different style of pretraining was employed in \cite{Wiseman:2016}. Instead of pretraining a seq2seq model with a different dataset they pretrained the model with a different loss function. This was necessary since the authors tried to integrate beam search into the loss function, but found that without first pretraining with the standard cross-entropy loss function the model was unable to learn anything with their new loss function.

\subsubsection{Additional Input Features} \label{sssec:priors}
In addition to the raw dialog turns we can integrate a plethora of other inputs into the seq2seq model. A number of attempts have been made since the birth of the encoder-decoder model in order to augment it with additional input features. The goal of these features is mainly to provide more information about the conversation or the context. In addition, models infused with additional features can learn to differentiate between different styles of dialog. By conditioning a seq2seq model on speaker information it can learn to output replies in the style of the speaker it was trained on \cite{Li:2016}. In this paper the authors input additional speaker embeddings into the decoder. These speaker embeddings are similar to word embeddings, basically representing the speakers for the utterances in the dataset. The additional speaker embedding vector is fed into the decoder RNN at each time-step jointly with the previously generated word. The result is a more consistent conversational model. Because of the speaker embeddings it learns general facts associated with the specific speaker. For example to the question \textit{Where do you live?} it might reply with different answers depending on the speaker embedding that is fed into the decoder. Additionally the authors have also experimented with speaker-addressee models. This is a simple extension to the previously discussed speaker model. Instead of simply feeding in speaker embeddings to the decoder RNN a weighted sum of the speaker and addressee embeddings is inputted. The addressee is the speaker to which the utterance is directed. The intuition behind this addition is that we talk in different styles depending on who we talk to (boss, friend, lover). Thus, the model not only responds differently with different speaker embeddings but it also conditions its response on the addressee embeddings. Hence, it might output a different reply for the same utterance and speaker embedding but different addressee embedding.

\begin{figure}[H]
	\label{fig:persona}
	\centering
	\includegraphics[width=0.9\textwidth]{pics/persona.png}
	\caption{A seq2seq model augmented with speaker embeddings \cite{Li:2016}}
\end{figure}

Similarly to this many other approaches were made to condition the response generation on various types of categories. For example in \cite{Xing_topic:2017} topic words are extracted from each utterance in the dataset. Then a seq2seq model is trained with these additional topic words together with the utterance they belong to. More specifically a separate attention mechanism is implemented that attends over the topic words. The two vectors generated from the normal utterance attention and the topic attention are fed into the decoder RNN at each time-step. With this additional topic information the model manages to produce more relevant replies to the topic of the conversation. A somewhat different approach to include topic information into the response generation process was employed in \cite{Choudhary:2017}. In this work three seq2seq models were trained separately on three datasets related to different topics (domains). Additionally a domain classifier based on logistic regression was trained to compute the probability that the utterance is related to a domain for all three domains. At test time, for an input utterance all the seq2seq models generate a reply and the domain classifier predicts a domain. Then a re-ranker takes the generated responses and the predicted domain probabilities and multiplies them together to determine the most probable reply-domain pair. Another example involves using emotion categories for post-response pairs \cite{Zhou:2017}. Here the authors first classify all of the post-response pairs in the dataset with an LSTM model into several emotion categories like happy, sad, angry, etc. Then they feed in these categories as real-valued vector representations into the decoder of an encoder-decoder model during training. Additionally a more complex memory based augmentation is proposed to the encoder-decoder model to better capture emotional changes in the response, which I don't detail here. In essence by feeding in these emotion categories we can generate a reply conditioned on a specific type of emotion. For example the response for the question \textit{How was your day?} will differ in style for different emotion categories.

A different approach is taken in \cite{Ghazvininejad:2017}, where the emphasis is put on taking into account relevant facts to an utterance. A seq2seq model is upgraded with a fact encoder which operates over a knowledge base which stores facts about different locations (eg. restaurants, theaters, airports). Before generating the reply to an encoded utterance a location or entity is extracted from it with keyword matching or named entity recognition. Then based on the location or entity, relevant facts are selected from the knowledge base, which are encoded by a memory network \cite{Sukhbaatar:2015}. Afterwards the vector representation from the encoded facts and the vector representation from the encoded utterance are summed and fed into the decoder RNN of the encoder-decoder model which generates the response. The authors used Twitter post-reply-reply 3-turn conversations which included a handle or hashtag about locations for which relevant facts were selected from a knowledge-base. With this approach the conversational model managed to produce more diverse and relevant replies if there was a location identified in the source utterance. Other similar methods involve leveraging information from knowledge bases as additional inputs which I describe in detail in Section~\ref{sssec:KB}. 

Other approaches try to integrate more features into the seq2seq model without using additional information. In a standard seq2seq model the only information the model has about the source utterance is through the vector representations of the words. To enrich the representation of the natural language utterance many other features have been proposed to be fed into the encoder RNN together with the word embeddings \cite{Sordoni:2015, Serban_MrRNN:2017,Serban:2017}. Such features include part-of-speech tags, dialog acts and n-gram word overlaps, just to name a few.

\subsubsection{Knowledge Bases and Copying} \label{sssec:KB}
Knowledge bases (KB) are powerful tools that can be used to augment conversational models. Since knowledge bases usually entail some kind of domain specific information, these techniques are mainly used for task-oriented dialog systems, which I talk about in more detail in Section~\ref{sssec:task}. In a KB we can store information related to the task at hand, for example information about nearby restaurants or about public transportation routes. Simple dictionaries or look-up-tables can be used to match an entity with information about it. Since KBs store information discretely, their integration with neural network based encoder-decoder models is not trivial.

Perhaps the most straightforward integration is proposed in \cite{Zhao:2017}. In this work an entity indexing step is introduced before feeding in the source utterance into a seq2seq model. This works by replacing locations or time specifying words with general tokens using named entity recognition or keyword matching. For example in the sentence \textit{I wan to go to Paris from New York}, the word \textit{Paris} is replaced with the token \textit{[LOC-1]} and the \textit{New York} is replaced with \textit{[LOC-0]}. The connection between the general token and the original word is stored in a table.  Then the encoder-decoder model produces a response that also uses general tokens for locations and times and a special placeholder token for the KB result. Finally, the general tokens are transformed back to actual words using the stored table and a KB is employed which uses these general tokens to search for a route between the two places and its output is incorporated in the response. Thus, we have an open-domain dialog system which is augmented with a task-oriented and robust part handling user requests related to finding routes between two locations.

\begin{figure}[H]
	\label{fig:KB}
	\centering
	\includegraphics[width=0.9\textwidth]{pics/KB.png}
	\caption{A HRED with attention, which I talk about in detail in Section~\ref{sssec:HRED}, upgraded with a KB component \cite{Zhao:2017}}
\end{figure}

A similar, but more complex approach is taken in \cite{Wen:2016}, where a KB augmented encoder-decoder model is used for the task of recommending restaurants. Here, besides a standard encoder RNN the source utterance is also processed with a belief tracker implemented as a convolutional neural network (CNN). I talk about convolutional neural networks applied to encoder-decoder models in Section~\ref{sssec:diffencdec}. Belief tracking is an important part of task-oriented spoken dialog systems \cite{Henderson:2015}. Here the belief tracker network produces a query for a MySQL database containing information about restaurants. The final input to the decoder RNN is the weighted sum consisting of the last state of the decoder RNN and a categorical probability vector from the belief tracker. Then the decoder outputs a response in the same way as in the previous example, with lexicalised general tokens. These tokens are then replaced with the actual information that they point to in the KB. Similarly in \cite{Ghazvininejad:2017} more general fact based KB is employed to augment an encoder-decoder model, which I talk about in detail in Section~\ref{sssec:priors}.

One of the most popular task-oriented datasets is also in the restaurant domain \cite{Joshi:2017}. Here special API calls are implemented to search over a KB. In order to call the API functions the dialog system has to first identify all the inputs by retrieving them from the user's utterances (eg. type of restaurant, price range). I describe this task in detail in Section~\ref{sssec:task}. A more general attempt to retrieve relevant information by the use of queries based on information probabilities is presented in \cite{Yin:2017}.

Named entity detection is also used together with a KB to solve a different problem in conversational modeling \cite{stalemate:2016}. In this work the authors try to solve the problem of users getting disinterested in the conversation with a dialog agent. When their model detects that the user is bored (eg. writes ...) the normal dialog system is replaced with a content introducing mechanism. More specifically the last couple of utterances are searched to retrieve any named entities. Then these entities are inputted to a KB, which contains associations between different entities, for example between a movie title and actor names in the movie. This information is then used to produce the response. Hence, the dialog system can introduce new relevant content based on the KB.

Another interesting line of research in conversational modeling is the integration of copying mechanisms in order to deal with out-of-vocabulary words \cite{Eric:2017,Gu:2016}. In \cite{Gu:2016} the probability of generating a word \(\bm{y}_t\) a time-step \(t\) is given by summing the probabilities from generate- and copy-modes. These probabilities are given by scoring functions. The scoring function of the generate-mode produces a score for each word in the vocabulary based on simple multiplication of the current decoder hidden state with a learned weight matrix. In contrast the scoring function for copy-mode produces scores for each OOV word \(\bm{x}_i\) in the source sentence. This scoring function is very similar to an attention mechanism:
\begin{equation}
f(\bm{x}_i)=\sigma(\bm{h}_{i}^{\mathsf{T}}W_c)\bm{s}_t
\end{equation}
where \(\bm{h}_{i}^{\mathsf{T}}\) is the hidden state of the encoder at step \(i\), \(\bm{s}_t\) is the hidden state of the decoder at the current time-step, \(W_c\) is a learned weight matrix and \(\sigma\) is a non-linear activation function. Using this copy-mode the model is able to efficiently handle OOV words and even integrate them into the response, which makes replies more diverse and unique.

\subsection{Different Approaches to Conversational Modeling} \label{ssec:33}
In this section I first describe hierarchical models used for building conversation agents in Section~\ref{sssec:HRED}. Then I talk about some approaches to integrate task-oriented conversations and goals with encoder-decoder models in Section~\ref{sssec:task}. Furthermore I present reinforcement learning based approaches in Section~\ref{sssec:RL}, that have seen some success when applied to the task of training conversational agents. Finally, I present encoder-decoder models that are very different from the standard RNN based seq2seq in Section~\ref{sssec:diffencdec}, but nonetheless have achieved state-of-the-art results.

\subsubsection{Hierarchical Models} \label{sssec:HRED}
In order to better represent dialog history a general hierarchical recurrent encoder-decoder (HRED) architecture was presented in \cite{Serban:2015}. The model consists of three different RNNs, the encoder RNN, the context RNN and the decoder RNN. First, \(k\) previous utterances of a conversation are encoded separately by the encoder RNN. This produces \(k\) separate context vectors by taking the last hidden state of the encoder RNN from each encoded utterance. Then these \(k\) hidden states are fed into the context RNN step by step, thus the it has to be unrolled for \(k\) steps. Next, the last hidden state from the context is used to initialize the decoder RNN. The decoder RNN and the decoding process is very similar to the seq2seq model.

The HRED differs from the basic encoder-decoder model by introducing a new level of hierarchy, the context RNN, which computes hidden states over entire utterances. With this approach the natural hierarchy of conversations is preserved by handling word sequences with the encoder RNN and utterance sequences with the context RNN which depends on the hidden representations produced by word-level encoding.

\begin{figure}[H]
	\label{fig:HRED}
	\centering
	\includegraphics[width=0.9\textwidth]{pics/HRED.png}
	\caption{The Hierarchical Recurrent Encoder-Decoder Model \cite{Serban:2015}}
\end{figure}
Since the introduction of the HRED model a number of works have used and augmented it \cite{Serban_VHRED:2017,Serban_MrRNN:2017,Serban:2017,Shen:2017,Li_adversarial:2017}. A proposed extension to the HRED model is to condition the decoder RNN on a latent variable which is sampled from the prior distribution at test time and the approximate posterior distribution at training time \cite{Serban_VHRED:2017}. These distributions can be represented as a function of previous sub-sequences of words. 

In \cite{Serban_MrRNN:2017} two HRED models are employed simultaneously. One HRED operates over coarse tokens of the conversation (eg. POS tags) to generate coarse predictions. Then the second HRED which takes as input the natural language utterances generates natural language predictions by conditioning on the predicted coarse tokens. This conditioning is done via concatenating the last hidden state of the decoder RNN of the coarse predictions with the current context RNN hidden state from the natural language HRED and feed it into the natural language decoder RNN.

In order to handle the two-party style of conversations, two separate hierarchical recurrent encoder (HRE) models are used in \cite{Shen:2017}. The HRE is similar to the HRED, the only difference being that it misses the decoder RNN. One of the HRE networks encodes only the utterances coming from one of the speakers, and the other HRE encodes the utterances of the other speaker. Thus at each turn the two networks produce two hidden context states which are concatenated and fed into a single decoder RNN, which produces the output prediction.

A natural extension to the original HRED model is to incorporate attention mechanisms which was explored in several works \cite{Yao:2015,Yao:2016,Xing:2017}. The simplest form of integrating attention with the HRED model is between the previous decoder RNN hidden state and the encoder RNN hidden states from the previous turn. This can be done in exactly the same way as in standard seq2seq models and has been explored in \cite{Yao:2015,Yao:2016}.

A more interesting approach is to make use of the hierarchical structure of the HRED and integrate attention hierarchically as well \cite{Xing:2017}. Accordingly the model in this work is called the hierarchical recurrent attention network (HRAN). There are two levels of attention employed. The word level attention mechanism computes vector representations over the hidden states of the encoder RNN (keys) and the previous hidden state of the decoder RNN (queries). Each utterance is encoded separately by the encoder RNN and word level attention is also computed separately for each utterance. The produced vector representations are fed into the utterance level encoder (context RNN). Then the utterance level attention mechanism computes a context vector based on the hidden states of the context RNN. The last step is to feed in this context vector into the decoder RNN at each step, which computes the output sequence predictions. 

An additional technique employed in the HRAN architecture is to use context RNN hidden states as input to the word level attention. More specifically the context RNN is implemented in a reverse order, meaning that if we have a sequence of utterances \((u_1,...,u_n)\), the encoder RNN first encodes \(u_n\), thus the utterance level encoder will also start with the vector representation from the last utterance in the conversation. Because the utterances are reversed the word level attention mechanism for each utterance can use as additional input the hidden state of the context RNN from the next utterance in the original order. Figure~\ref{fig:HRAN} depicts the HRAN with all the components mentioned above.
\begin{figure}[H]
	\label{fig:HRAN}
	\centering
	\includegraphics[width=0.9\textwidth]{pics/hran.png}
	\caption{The Hierarchical Recurrent Attention Network \cite{Xing:2017}}
\end{figure}




\subsubsection{Task-Oriented Dialog Systems} \label{sssec:task}
As I have discussed in Section~\ref{ssec:21} task-oriented dialog agents are a sub-type of conversational models. They are more narrow with regard to language understanding and conversation topics, but they make up with robustness, making them better suited for commercial deployment. In task-oriented dialog systems there is usually a general task defined, and a goal to be achieved through conversation. A common property of task-oriented systems is that they learn from smaller and task-specific datasets. Since it is usually not required to generalize to other domains, learning on smaller, but more focused datasets gives much better performance. Furthermore, KB components described in Section~\ref{sssec:KB} are often used in combination with neural network models. KBs are an essential component of task-oriented dialog systems since they provide the model with accurate and rich information. Thus, neural network models only have to learn how to carry general conversations, how to form queries to KBs and how to incorporate the result from KB look-ups. Also, rule-based systems often perform equally well at task-oriented conversations, since only certain types of questions related to the task are expected from the user.

A prevalent task and goal is the recommendation of restaurants to users searching for specific types of cuisine. One of the most popular benchmark tasks is situated in this domain, named bAbi \cite{Bordes:2016,Joshi:2017,bAbi}. Since this is a simulated dataset a rule-based system can achieve 100\(\%\) task success rate. Nonetheless it is an important benchmark for neural conversational models. The task is further divided into 5 sub-tasks to measure the performance on individual parts of dialogs. Task 1 consists of dialog conducted until the first API call. Here the dialog agent has to gather sufficient information from the user's utterances in order to call an API function. This function usually takes as input the type and location of the restaurant, for how many people should the reservation be made, and the price range. Based on these inputs it automatically returns a list of candidate restaurants using a KB. Task 2 measures the ability of the dialog agent to handle instances when the user forgot to mention an option or would like to change one. Then, in Task 3 the agent presents a possible candidate and converses with the user until the final restaurant is selected by the user. Finally, Task 4 measures the ability of the dialog agent to provide additional information about the restaurant like an address or a phone number. Task 5 measures the overall success of the full dialog. Different approaches to using KBs with neural models within the restaurant domain have been proposed \cite{Wen:2016,Eric:2017,Williams:2017}. Other bAbi tasks have also been explored with neural models augmented with KBs \cite{Williams:2017,Li_HIL:2016}, for example dialog based reasoning \cite{Weston:2015} or movie recommendation tasks \cite{Miller:2016,Dodge:2015}.

A more general recommendation based dialog system is explored in \cite{Yin:2017}. The system presented in this work uses an information extraction component which takes in a query and returns results from a search engine.

\begin{figure}[H]
	\label{fig:babi}
	\centering
	\includegraphics[width=0.7\textwidth]{pics/babi.png}
	\caption{The 5 bAbi tasks for restaurant reservations. The dialog agent's utterances are in blue, the user's utterances are in green and API function calls are in red \cite{Bordes:2016}}
\end{figure}
A number of papers address the problem of integrating open-domain and task-specific approaches into one conversational model \cite{Zhao:2017,Yu:2017,Akasaki:2017}. I have also described some of these in Section~\ref{sssec:KB}, since they usually use a knowledge base. Essentially the bAbi toy tasks mentioned above do not represent real-life interactions between users and task-oriented dialog systems. In order for conversational models to remain robust they have to able to handle out of domain utterances. Otherwise users might get annoyed with the program, for example many rule-based systems might resort to output responses like \textit{Sorry, I couldn't understand that, but I am happy to talk about movies.} in order to get the user back on track. One approach to handle these issues is to build a model that is able to differentiate between in-domain and non-task user utterances. Then two different models trained separately on open-domain and task-specific datasets can produce response candidates and a scoring function can output the most probable response \cite{Akasaki:2017}.


\subsubsection{Reinforcement Learning} \label{sssec:RL}
Reinforcement learning (RL) \cite{Sutton:1998} is a type of learning framework which has seen increasing success recently \cite{Mnih:2013,Mnih:2015,Silver:2016}. RL performs very well in tasks where we don't have defined loss functions or don't know what the gold truths are. Instead, learning is implemented using a reward which is automatically given to the model based on its state. A general visual description of the reinforcement learning framework can be seen in Figure~\ref{fig:RL}.

\begin{figure}[H]
	\label{fig:RL}
	\centering
	\includegraphics[width=0.7\textwidth]{pics/RL.png}
	\caption{The reinforcement learning framework \cite{Sutton:1998}}
\end{figure}

In RL an agent is some kind of function consisting of parameters that are optimized with a learning algorithm. The agent receives as input the state of the environment and can output actions based on it. The goal is to maximize the expected reward through a good combination of actions. The environment is the task or setting in which the agent is situated. One of the most popular applications of RL is games, since it's not known what are the correct actions in all steps of game \cite{Mnih:2013}. Instead, games have a clear end-goal and a learning agent can be rewarded based on whether it achieves the end-goal through its actions. An \textit{episode} refers to a sequence of actions and states \((s_1,a_1,...,s_t,a_t,s_T)\) until the agent reaches a terminal state \(s_T\). The terminal state is referred to the state in which the agent receives a reward for the episode. This process, often called a markov decision process (MDP) consists of the triplet \((\bm{S},\bm{A},R)\), where \(\bm{S}\) is the set of states, \(\bm{A}\) is the set of actions and \(R\) is the reward for the episode \cite{Kandasamy:2017}. A policy \(\pi\) is a function that the agent implements in order to select an action to a given state. Stochastic policies \(\pi(a|s)\) represent the probability of an agent executing action \(a\) in state \(s\).

The application of RL to neural conversational models is fairly straightforward. For a dialog agent the observed state at time-step \(t\) consists of the input sentence and what the agent has already outputted so far. The actions are the words in the vocabulary, since at any time-step it can only produce a word from the vocabulary. When the agent finishes generating the reply a reward is observed, which can be just the difference between the generated sentence and the gold truth or other more sophisticated rewards, which I describe in the next paragraph. Thus, the agent has to maximize this reward through a sequence of actions, generating one word at each time-step. The policy according to which the agent takes actions can be implemented as a normal seq2seq model. Generally, to train RL dialog agents the future reward of the episode has to be estimated at each time-step in order to get a reward for each action. This is usually done via the REINFORCE algorithm \cite{Williams:1992}. However, this approach is not perfect for dialog agents. Since rewards can only be observed at the end of an episode, all actions in that episode get the same reward. This means that if a response is of poor quality, like answering \textit{I don't know} to the question \textit{What's your name?} even the word \textit{I}, which is mostly neutral will receive a negative reward.  Instead as is customary in RL literature we would like to give a different reward for each action. A possible solution is offered in \cite{Kandasamy:2017} using batch policy gradient methods. A different solution uses Monte Carlo search in order to sample tokens for partially decoded sentences \cite{Li_adversarial:2017}. If we have a reward function which is similar to a loss function we can train the agents by simple stochastic gradient descent.

Reward functions are often hand-crafted, meaning that we have to mathematically formulate important properties of conversations. In \cite{Li_RL:2016} the weighted sum of three different reward functions is used. The first one attempts to make responses easy to answer to. A set of dull and boring responses \(S\) is constructed, and if the response to the current action \(a\) is similar to these than the current action is given a negative reward:
\begin{equation}
r_1=-\frac{1}{N_S}\sum_{s\in S}\frac{1}{N_s}\log{p_{seq2seq}(s|a)}
\end{equation}
where \(N_S\) denotes the cardinality of \(S\) and \(N_s\) denotes the number of tokens in \(s\). \(p_{seq2seq}\) represents the probability output from a standard seq2seq model. The second reward attempts to capture the information flow in a conversation. More specifically, each utterance should contribute with new information and it shouldn't be repetitive compared to previous ones. In order to achieve such a reward the semantic similarity between sentences can be penalized by
\begin{equation}
r_2=-\log{\cos{\frac{\bm{h}_{p_{i}} \bm{h}_{p_{i+1}}}{||\bm{h}_{p_{i}}|| ||\bm{h}_{p_{i+1}}|| }}}
\end{equation}
where \(\bm{h}_{p_{i}}\) and \(\bm{h}_{p_{i+1}}\) denote hidden representations from the encoder RNN for two consecutive turns \(p_i\) and \(p_{i+1}\). The final reward function focuses on semantic coherence. This rewards generated responses that are coherent and grammatical by taking into account the mutual information between action \(a\) and previous dialog turns:
\begin{equation}
r_3=\frac{1}{N_a}\log{p_{seq2seq}(a|q_i,p_i)}+\frac{1}{N_{q_i}}\log{p_{seq2seq}^{backward}(q_i|a)}
\end{equation}
where \(p_{seq2seq}(a|q_i,p_i)\) denotes the probability computed by a seq2seq model of generating response \(a\) given previous dialog utterances \([p_i,q_i]\). \(p_{seq2seq}^{backward}\) is a seq2seq model trained with swapped targets and inputs as discussed in Section~\ref{sssec:functions}. \(N_a\) and \(N_{q_i}\) are the number of tokens in utterances \(a\) and \(q_i\) respectively.

Similarly in \cite{Yu:2017} a linear combination of four reward functions is used. In this work the functions implemented tried to address turn-level appropriateness, conversational depth, information gain and conversation length in order to produce better quality responses. In \cite{Yao:2016} the reward function is based on calculating the sentence level inverse document frequency \cite{Salton:1988} of the generated response.

However, hand-crafting reward functions is not ideal, since we might not know exactly what conversational properties should we capture and how to formulate them mathematically in an expressive way. A solution to this problem is to let agents figure out the appropriate reward by themselves, or use another agent to assign the reward. This has been explored before by using an adversarial approach \cite{Li_adversarial:2017}. Generative adversarial networks \cite{Goodfellow:2014} consist of two networks competing with each other. A generator network (in this case a seq2seq model) tries to generate responses that mimic the training data and a discriminator network (implemented as an RNN based binary classifier in this case) tries to figure out whether the generated response came from the dataset or the generator network. Both of the networks are trained on whether the discriminator guessed correctly. Hence, the reward for the generator network comes from the discriminator network and represents whether the generator managed to fool the discriminator.

In \cite{Havrylov:2017} and \cite{Kottur:2017} a different line of research is explored using similar methods. The setting is still conversational however conversation arises from a two-agent collaborative game, and it is argued whether natural language-like dialog can arise from such a setting. The sender agent (implemented as an RNN) receives as input an image and outputs a message, while the receiver agent (implemented as an RNN) receives as input this message and a set of pictures \(K\) and outputs a probability distribution over the pictures. The probability for a picture \(k\in K\) gives the likeliness of that picture being the one shown to the sender agent. Thus, the sender agent has to formulate a message which contains enough information so that the receiver can choose between the pictures. This is a collaborative RL setting, since both agents receive the reward related to whether the receiver agent choose the right picture.

The above mentioned approaches are all off-line, since the reward is given through a predetermined reward function, and learning is based on a training dataset. However on-line reinforcement learning has also been explored in a conversational setting \cite{Li_HIL:2016}. In on-line learning the reward for generating a response is given by a human teacher interacting with the agent. The teacher can rate the response of the system on a scale and assign a numerical reward to it. This approach is very advantageous when we don't have a labeled dataset of dialogs. The drawback is that human supervision is required.

\subsubsection{Different Encoder-Decoder Models} \label{sssec:diffencdec}
% don't write about attention is all you need here, because you will describe it in ssec:41
So far the underlying architecture of encoder-decoder models was based on recurrent neural networks. However, attempts have been made towards using other types of neural networks as well. Recently, seq2seq models based solely on convolutional neural networks (CNNs) have achieved very good results in the field of NMT \cite{Kaiser:2016,Kaiser:2017,ByteNet:2016,ConvS2S:2017}. CNNs are a variant of feed-forward neural networks, that use a shared-weights architecture. They are the dominating architecture in the image processing domain \cite{Imagenet:2012}. CNN encoder-decoder models have not yet been applied to conversational modeling, but since the original RNN based seq2seq also originated from NMT it is important to mention them. The appeal of CNN based models is that they are much faster to compute than RNNs, because they can be better parallelized, since they don't use recurrence.

A fully attention based encoder-decoder model was presented in \cite{Vaswani:2017}, which I talk about in detail in Section~\ref{ssec:41}. A more general encoder-decoder model was presented in \cite{Kaiser_one_model:2017}. This model made use of all kinds of neural architectures like CNNs, attention mechanisms and sparsely-gated mixture-of-experts \cite{MoE:2017}. The generality and level of abstraction employed in the model makes it possible to achieve good results across many different tasks and even modalities (audio, vision, text) by being trained simultaneously for all these tasks and using the same parameters, only varying the inputs. A general diagram of the model can be seen in Figure~\ref{fig:multimodel}. Inputs coming from different modalities are encoded into common abstract representations using modality nets. The architecture of the modality net depends on the type of input. Additionally modality nets are also used to transform outputs into correct modality data. Between these modality nets a single encoder-decoder model transforms input representations to output representations. The decoder is autoregressive, meaning that it takes as input its previous output as well. The architecture of this model is very different from a standard RNN or CNN based seq2seq model, since it uses several types of building blocks like attention, mixture of experts and convolutional layers. I don't detail it here further, but the exact architecture can be found in Section 2 of the paper \cite{Kaiser_one_model:2017}.

\begin{figure}[H]
	\label{fig:multimodel}
	\centering
	\includegraphics[width=0.6\textwidth]{pics/multimodel.png}
	\caption{A high-level visualization of the MultiModel \cite{Kaiser_one_model:2017}}
\end{figure}

\subsection{Criticism} \label{ssec:problems}
A conversation is a complex information exchanging process. In this section I argue, based on recent literature, that current approaches to conversational modeling are not entirely suitable for the task. First, in Section~\ref{sssec:datasets} I show why some of the datasets used to train dialog agents are inherently not appropriate. Then, I argue that the standard loss function used to train conversational models fails to capture an important property of conversations in Section~\ref{sssec:loss_function}. The memory of dialog agents is also an issue that I talk about in Section~\ref{sssec:memory}. Finally, in Section~\ref{sssec:metrics} I show why some of the standard evaluation metrics used to compare and validate different models perform poorly and do not provide meaningful comparisons.
	
\subsubsection{Datasets} \label{sssec:datasets}
%dan mizulescu argument about movie lines
A good overview of datasets available for training dialog agents is presented in \cite{Serban_survey:2015}. In this work I mainly focus on open-domain dialog agents. For such agents large quantities of data is necessary in order to be able to learn about various topics and properties of language. Large datasets are usually noisy, as is the case with the OpenSubtitles dataset \cite{OpenSubtitles:2016,opensubtitles}, where the correct turn segmentation is not even known and usually it's assumed that alternate sentences are spoken by alternate people \cite{Vinyals:2015}. Furthermore, movie dialogs do not represent real, natural conversation since they are hand-crafted and often the conversations themselves are goal-oriented and involve outside factors related to the current scene in the movie. A good argument is given in \cite{Danescu:2011}: \textit{"For example, mundane phenomena such as stuttering and word repetitions are generally nonexistent on the big screen. Moreover, writers have many goals to accomplish, including the need to advance the plot, reveal character, make jokes as funny as possible, and so on, all incurring a cognitive load."}.

The other prevalent type of dataset is based on messageboards, post-reply websites, like the Ubuntu dialog corpus \cite{Lowe:2015} or Twitter \cite{Shang:2015,Li:2016,Jena:2017}. The underlying problem with these datasets is that they are not actual one on one conversations but rather a post to which many people can reply and thus there aren't really many-turn dialogs. Secondly, the conversations are public, which in my opinion are not as natural as private ones. Since the goal is to build an open-domain chatbot that can hold private conversations, the data that it was trained on should also be similar in nature.

\subsubsection{The Loss Function} \label{sssec:loss_function}
In seq2seq models the underlying standard function that is used for training, the loss or objective function is based on calculating how different the predicted response probabilities and the gold truth are. This function was borrowed from NMT, where it performs well, since for each input sentence there is usually a well-defined output translation. However, in conversations a source utterance can have a multitude of suitable replies, which can be very different from each other semantically. For example the reply to \textit{How was your day?} can be as simple as \textit{Okay.} or as long as a whole paragraph describing events that happened during the day. Since the task of conversational modeling is so ambiguous the standard loss function is not suited for training good chatbots \cite{Vinyals:2015,Li:2015}. In many works it has been shown that dialog agents tend to output generic responses like \textit{I don't know} \cite{Vinyals:2015,Serban:2015,Li:2015,Li:2016,Jena:2017}. My presumption is that it is precisely because of the loss function that the models learn to output safe and generic responses. Since the task of conversational modeling is ambiguous and even in a training set there are plenty of different replies to the same utterance the loss function makes the model learn an average of these responses. Utterances can be represented by the word embeddings that make them up, many dimensional real-valued representations. Hence, if there are multiple semantically different replies to the same source utterance, in the multi-dimensional space their average will point close to the origin. The origin consists of neutral and generic replies, since these have been seen by the model as replies for many different source utterances. Thus, the model will learn to output these neutral responses, because it was trained with a loss function that tries to average out ambiguity.

Attempts have been made to address these issues and to define better loss functions, which I presented in Section~\ref{sssec:functions}. I also propose my own ideas about solving the loss function issue in Section~\ref{ssec:61}.

\subsubsection{Memory} \label{sssec:memory}
A more specific problem with current conversational models is that they don't have any memory. While architectures have been devised in order to take into account previous turns, they are still limited. For example they can't take into account hundreds of previous turns. A user would expect from a chatbot that once he/she talks about something, the chatbot will mostly remember the dialog, as is normal in human-human conversations. The chatbot should be able to learn new things about the user in order to make the experience personalized. Currently there aren't any attempts to address this issue, since it is not conversational, but rather it arises from the deployment of dialog systems to the real world. I propose a novel idea that could solve this issue in Section~\ref{ssec:62}.

\subsubsection{Evaluation Metrics} \label{sssec:metrics}
% show graphs of how badly standard metrics correlate
It has been shown that standard metrics like perplexity and Bleu, which perform well in NMT, don't correlate at all with human judgment for evaluation of dialog agents \cite{Tao:2017,Liu:2016}. Figure~\ref{fig:bleu} depicts the correlation of Bleu with human judgment in evaluating a seq2seq model trained on two different datasets. I believe that the reason for this is similar to the loss function issue. Namely, these metrics assume that there is only one gold truth response for each source utterance, because they are based on measuring the difference between the predicted and the gold truth response. Since this is not the case, they will assign low scores to responses that were deemed natural by human judges, but aren't similar to the gold truth response.
\begin{figure}[H]
	\label{fig:bleu}
	\centering
	\includegraphics[width=0.6\textwidth]{pics/bleu.png}
	\caption{Bleu score correlation with human scores \cite{Liu:2016} on a Twitter corpus and on the Ubuntu dialog corpus \cite{Lowe:2015}}
\end{figure}
Furthermore, while human evaluation is one of the most accurate metrics it still has its drawbacks. Most importantly it is not automatic and thus can be costly and can not be standardized.

Attempts have been made in order to design better automatic evaluation metrics based on neural networks \cite{Tao:2017,Lowe:2017,Li_adversarial:2017}. Since they are neural network based they can be trained with labeled data. They receive a source utterance and the reply to it as input and they have to output a score. Then, this score can be compared with the true score assigned by a human annotator and thus the model learns from its errors. As can be seen in Figure~\ref{fig:adem_ruber} these metrics indeed correlate better with human scoring.
\begin{figure}[H]
	\label{fig:adem_ruber}
	\centering
	\includegraphics[width=0.8\textwidth]{pics/adem_ruber.png}
	\caption{Correlation of ADEM (left) \cite{Lowe:2017} and RUBER (right) \cite{Tao:2017} with human scorers \cite{Lowe:2015}}
\end{figure}

\subsection{Summary} \label{ssec:summary}
In this section I presented an in-depth survey of recent literature related to conversational agents. I talked about additional, but essential details of encoder-decoder models, like the context of conversations, objective functions and evaluation methods in Section~\ref{ssec:31}. Then, in Section~\ref{ssec:32} I gave an overview of different techniques used to augment the performance of seq2seq models, like attention, pretraining, additional input features, knowledge bases and copying. I finished the literature survey by describing different encoder-decoder architectures in Section~\ref{ssec:33}, like hierarchical models, task-oriented systems, reinforcement learning based agents and other encoder-decoder models that are not based on RNNs.

Finally, in Section~\ref{ssec:summary} I argued that some of the current approaches to build conversational models are inherently not suitable for the task. I presented arguments related to current datasets not providing natural conversational data and that the standard loss function used to train conversational models fails to capture important properties of dialogs. I finished my criticism by showing that current dialog agents lack memory, an essential part of conversations and why standard evaluation metrics are unsuitable to evaluate dialog agents.
    

\newpage\section{Experiments} \label{sec:experiments}
In this section I present my experiments with the Transformer encoder-decoder model, which I describe in detail in Section~\ref{ssec:41}. I experiment with training it using conversational data, which hasn't been done before, because the architecture is very recent and it was originally developed for NMT \cite{Vaswani:2017}. In Section~\ref{ssec:42} I give a brief overview of the datasets I used for training. Finally, in Section~\ref{ssec:43} I give a detailed description of the training setups that I ran.
\subsection{The Transformer Model} \label{ssec:41}
\subsection{Datasets} \label{ssec:42}
\subsection{Training Details} \label{ssec:43}

\newpage\section{Results} \label{sec:results}
\subsection{Quantitative Analysis} \label{ssec:51}
\subsection{Qualitative Analysis} \label{ssec:52}

\newpage\section{Future Work} \label{sec:future}
\subsection{Ideas Towards Solving The Loss Function Issue} \label{ssec:61}
\subsection{Temporal Conditioning and Memory} \label{ssec:62}
\subsection{Additional Ideas} \label{ssec:63}

\newpage\section{Conclusion} \label{sec:conclusion}


\bibliographystyle{apalike}
\newpage\bibliography{chatbots}
\end{document}
