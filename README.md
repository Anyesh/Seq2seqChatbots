# Seq2seqChatbots
 
This repository contains the code that I have written for experiments described in [this](https://tdk.bme.hu/VIK/DownloadPaper/asdad) paper. I made my own problem, hparams and model registrations to the [tensor2tensor](https://github.com/tensorflow/tensor2tensor) library in order to try out different datasets with the [Transformer](https://arxiv.org/abs/1706.03762) modell for training dialog agents. The folders in the repository contain the following content:
* **docs**: Latex files and pictures required to generate my [paper](https://tdk.bme.hu/VIK/DownloadPaper/asdad). Also check my research [proposal](https://github.com/ricsinaruto/Seq2seqChatbots/blob/master/doc/research_proposal.pdf) for a detailed description of my current research interests.
* **t2t_csaky**: This folder contains all the code that I have written, more detailed description can be found lower.
* **train_dir**: Here you can find inference outputs from the various trainings that I have run.
* **wiki_images**: Contains images used for the [wiki](https://github.com/ricsinaruto/Seq2seqChatbots/wiki/Chatbot-and-Related-Research-Paper-Notes-with-Images), where I write about more than 100 publications related to chatbots.

## Quick Guide
First, install all the required packages in your python environment:
```
pip install -r requirements.txt
```

### Data
In order to download and preprocess the data and generate source and target pairs, the following command can be used from this directory:
```
t2t-datagen --t2t_usr_dir=t2t_csaky --data_dir=$Path-to-data-dir --problem=$Name-of-problem
```
Where:
* **$Path-to-data-dir**: The path to the directory where you want to generate the source and target pairs, and other data. The dataset will be downloaded one level higher from this directory into a *raw_data* folder.
* **$Name-of-problem**: This is the name of a registered problem that tensor2tensor needs. Currently I have 3 registered problems:
  * *[opensubtitles_chatbot](https://github.com/ricsinaruto/Seq2seqChatbots/blob/master/t2t_csaky/problems/opensubtitles_chatbot.py)*: This problem can be used to work with the [OpenSubtitles](http://opus.nlpl.eu/OpenSubtitles2018.php) dataset. Since there are several versions of this dataset, you can specify the year of the dataset that you want to download with the *dataset_version* property inside the class.
  * *[cornell_chatbot_basic](https://github.com/ricsinaruto/Seq2seqChatbots/blob/master/t2t_csaky/problems/cornell_chatbots.py)*: This problem implements the [Cornell Movie-Dialog Corpus](https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html).
  * *[cornell_chatbot_separate_names](https://github.com/ricsinaruto/Seq2seqChatbots/blob/master/t2t_csaky/problems/cornell_chatbots.py)*: This problem uses the same Cornell corpus, however the names of the speakers and addressees of each utterance are appended, resulting in source utterances like below. Thus the size of the vocabulary containing these names can be specified through the *targeted_name_vocab_size* property inside the CornellChatbotSeparateNames class.
    > BIANCA_m0 what good stuff ?  CAMERON_m0
  * *character_chatbot*: This is a general character-based problem that works with any dataset. Before using this, the .txt files generated by any of the problems above have to be placed inside the data directory, and after that this problem can be used to generate tensor2tensor character-based data files.
* Further properties that affect the data generation and can be set in each of the classes:
  * *targeted_vocab_size*: Size of the vocabulary that we want to use for the problem. Words outside this vocabulary will be replaced with the <unk> token.
  * *targeted_dataset_size*: Number of utterance pairs, if we don't want to use the full dataset.
  * *dataset_split*: Specify a train-val-test split for the problem.

### Training
If you have generated the data files, you can train any model offered by tensor2tensor, using this command:
```
t2t-trainer \
  --t2t_usr_dir=t2t_csaky \
  --generate_data=False \
  --data_dir=$Path-to-data-dir \
  --problems=$Name-of-problem \
  --model=$Name-of-model \
  --hparams_set=$Name-of-hparams \
  --output_dir=$Path-to-train-dir
  ```






### Sample conversations from the various trainings
S2S is a baseline seq2seq model from [this](https://arxiv.org/pdf/1506.05869.pdf) paper, Cornell is the Transformer model trained on Cornell data, Cornell S is similar, but trained with speaker annotations in addition. OpenSubtitles is the Transformer trained with OpenSubtitles data, and OpenSubtitles F, is the previous training finetuned (further trained) on Cornell speaker annotated data.
<a><img src="https://github.com/ricsinaruto/Seq2seqChatbots/blob/master/docs/tdk/pics/general_questions.png" align="top" height="550" ></a>




### Decoding
You can decode from the trained model interactively, using the command below. Also, for all 4 trainings that I ran, I uploaded the checkpoint files [here](https://mega.nz/#!bckTiS6Z!3CJxsl4AyR1W6eUnJ6Viq_cKMhhMh82cFlmA9xbotpo) so you can try them out without needing to train. Just copy the checkpoint files to the appropriate folders in train_dir, and provide the folder which you want for the output_dir flag.
```
t2t-decoder \
  --t2t_usr_dir=t2t_csaky \
  --data_dir=data_dir/cornell_movie_database \
  --problems=chatbot_cornell32k \
  --model=transformer \
  --hparams_set=chatbot_cornell_base \
  --output_dir=train_dir/base_trf_cornell
  --decode_interactive
```


#### Run this command to train a simple lstm based seq2seq model
```
t2t-trainer --t2t_usr_dir=t2t_csaky --generate_data=False --data_dir=data_dir/opensubtitles_62M_sentences --problems=chatbot_opensubs100k --model=lstm_seq2seq --hparams_set=chatbot_lstm_batch_128 --output_dir=train_dir/base_lstm_seq2seq_opensubs --train_steps=800000 --keep_checkpoint_max=2 --keep_checkpoint_every_n_hours=1
```

If no out of memory (OOM) error is thrown try to increase the number in the hparams_set flag until an OOM error is thrown and then select the biggest batch that didn't throw OOM errors from these options: [128, 256, 512, 1k, 2k, 4k, 8k].

#### Run this command to train the [transformer](https://arxiv.org/abs/1706.03762) model
```
t2t-trainer --t2t_usr_dir=t2t_csaky --generate_data=False --data_dir=data_dir/opensubtitles_62M_sentences --problems=chatbot_opensubs100k --model=transformer --hparams_set=chatbot_transformer_batch_2k --output_dir=train_dir/base_transformer_opensubs --train_steps=800000 --keep_checkpoint_max=2 --keep_checkpoint_every_n_hours=1
```

If no out of memory (OOM) error is thrown try to increase the number in the hparams_set flag until an OOM error is thrown and then select the biggest batch that didn't throw OOM errors from these options: [2k, 4k, 8k, 16k, 32k]




###### If you require any help with running my code or if you want the files of the trained models, just contact me via e-mail and I will make them available.
