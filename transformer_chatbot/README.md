# Transformer Chatbot Modell
##### In order to run this project you have to install tensorflow 1.3 with gpu support and the [tensor2tensor](https://github.com/tensorflow/tensor2tensor) module.
I created my own registrations for the [tensor2tensor](https://github.com/tensorflow/tensor2tensor) framework in the t2t_csaky folder. I created a new problem using the [Cornell Movie-Dialogs Corpus](https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html).
If you require any help with running my code or if you want the files of the trained models, just contact me via e-mail and I will make them available.

#### Dataset files are already generated, ready for training and can be downloaded from [here](https://mega.nz/#!vJ0iRRYT!GzsFbihzdqw-H-2KDgIxL3_H7UhKmLaJlORBCIZlGDI) (put the data_dir folder in the transformer_chatbot folder of this repo).
* [cornell_movie_database](https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html) folder contains data generated by using all utterance pairs from the movie dialogs.
* cornell_movie_database_with_speakers folder contains same utterance pairs as above but with speaker and adressee annotations at the start and end of the source utterance.
* [opensubtitles_2M_sentences](http://opus.lingfil.uu.se/OpenSubtitles.php) folder contains all utterance pairs (2 million) extracted from the original Opensubtitles dataset.
* [opensubtitles_62M_sentences](http://opus.lingfil.uu.se/OpenSubtitles2016.php) folder contains 62 million utterance pairs extracted from the 2016 version of the Opensubtitles dataset.

After downloading the data files you can train the [transformer](https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py) model for example with [Cornell Movie data](https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html) with the following command:
```
t2t-trainer \
  --t2t_usr_dir=t2t_csaky \
  --generate_data=False \
  --data_dir=data_dir/cornell_movie_database \
  --problems=chatbot_cornell32k \
  --model=transformer \
  --hparams_set=chatbot_cornell_base \
  --output_dir=train_dir/transformer_with_cornell_data
```

You can decode from the model interactively:
```
t2t-decoder \
  --t2t_usr_dir=t2t_csaky \
  --data_dir=data_dir/cornell_movie_database \
  --problems=chatbot_cornell32k \
  --model=transformer \
  --hparams_set=chatbot_cornell_base \
  --output_dir=train_dir/transformer_with_cornell_data
  --decode_interactive
```
##### The train_dir folder contains sample output responses from the various trained models to the utterances found [here](https://github.com/ricsinaruto/Seq2seqChatbots/blob/master/transformer_chatbot/NCM_examples/NCM_examples.txt).
##### The NCM_examples folder contains different types of input utterances, from example speaker annotated ones.

### Detailed guide to train tensor2tensor models with Opensubtitles data
Prerequisites:
cudnn 6.0

Steps:
* pip3 install tensorflow-gpu (need version 1.3)
* pip install tensor2tensor
* git clone https://github.com/ricsinaruto/Seq2seqChatbots
* cd Seq2seqChatbots/transformer_chatbot/
* In this directory download and unzip data from [here](https://mega.nz/#!vJ0iRRYT!GzsFbihzdqw-H-2KDgIxL3_H7UhKmLaJlORBCIZlGDI)

#### Run this command to train a simple lstm based seq2seq model
```
t2t-trainer --t2t_usr_dir=t2t_csaky --generate_data=False --data_dir=data_dir/opensubtitles_62M_sentences --problems=chatbot_opensubs100k --model=lstm_seq2seq --hparams_set=chatbot_lstm_batch_128 --output_dir=train_dir/base_lstm_seq2seq_opensubs --train_steps=800000 --keep_checkpoint_max=2 --keep_checkpoint_every_n_hours=1
```

If no out of memory (OOM) error is thrown try to increase the number in the hparams_set flag until an OOM error is thrown and then select the biggest batch that didn't throw OOM errors from these options: [128, 256, 512, 1k, 2k, 4k, 8k].

#### Run this command to train the [transformer](https://arxiv.org/abs/1706.03762) model
```
t2t-trainer --t2t_usr_dir=t2t_csaky --generate_data=False --data_dir=data_dir/opensubtitles_62M_sentences --problems=chatbot_opensubs100k --model=transformer --hparams_set=chatbot_transformer_batch_2k --output_dir=train_dir/base_transformer_opensubs --train_steps=800000 --keep_checkpoint_max=2 --keep_checkpoint_every_n_hours=1
```

If no out of memory (OOM) error is thrown try to increase the number in the hparams_set flag until an OOM error is thrown and then select the biggest batch that didn't throw OOM errors from these options: [2k, 4k, 8k, 16k, 32k]

